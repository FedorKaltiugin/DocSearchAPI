{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f617fb-1eb0-4aac-8d47-b17a2a82afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Fedor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')  \n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d366f896-79ef-4727-bdd1-6479055f5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\", split=\"test\")\n",
    "\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f00b684-68ec-4e89-80ac-c320a5d9d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['Was Abraham Lincoln the sixteenth President of the United States?', 'Did Lincoln sign the National Banking Act of 1863?', 'Did his mother die of pneumonia?', \"How many long was Lincoln's formal education?\", 'When did Lincoln begin his political career?'], 'answer': ['yes', 'yes', 'no', '18 months', '1832'], 'id': [0, 2, 4, 6, 8]}\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "empty_questions = 0\n",
    "empty_answers = 0\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    indices = range(i, min(i + batch_size, len(dataset)))\n",
    "    chunk = dataset.select(indices)\n",
    "    print(chunk[:5])\n",
    "    df_chunk = chunk.to_pandas()\n",
    "\n",
    "    empty_questions += df_chunk[\"question\"].isna().sum()\n",
    "    empty_answers += df_chunk[\"answer\"].isna().sum()\n",
    "\n",
    "print(empty_questions)\n",
    "print(empty_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e0a344-35ae-461c-be36-f1606fcaeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-text method - BM25\n",
    "def bm25_search(query, dataset, top_n=5, batch_size=1000):\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    all_scores = []\n",
    "\n",
    "    all_questions = dataset['question']\n",
    "    all_answers = dataset['answer']\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        chunk_questions = all_questions[i:i + batch_size]\n",
    "        tokenized_passages = [word_tokenize(q.lower()) for q in chunk_questions]\n",
    "\n",
    "        bm25 = BM25Okapi(tokenized_passages)\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        global_indices = list(range(i, min(i + batch_size, len(dataset))))\n",
    "        all_scores.extend(zip(global_indices, scores))\n",
    "\n",
    "    sorted_scores = sorted(all_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    results = []\n",
    "    for idx, score in sorted_scores:\n",
    "        results.append({\n",
    "            \"score\": round(score, 2),\n",
    "            \"question\": all_questions[idx],\n",
    "            \"answer\": all_answers[idx]\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5644acc-43b7-40b4-adef-27785745613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector method - Sentence-BERT\n",
    "bert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    batch_questions = dataset['question'][i:i + batch_size]\n",
    "    batch_embeddings = bert_model.encode(batch_questions, batch_size=64)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "def bert_search(query, dataset, embeddings, top_n=5):\n",
    "    query_embedding = bert_model.encode([query])\n",
    "    scores = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        idx = int(idx)\n",
    "        results.append({\n",
    "            \"score\": round(scores[idx], 2),\n",
    "            \"question\": dataset['question'][idx],\n",
    "            \"answer\": dataset['answer'][idx]\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8185b9-d6e0-4372-b4fb-64c6123f4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid method - BERT + BM25\n",
    "def normalize_scores(score_dict):\n",
    "    scores = list(score_dict.values())\n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    if max_score == min_score:\n",
    "        return {k: 0.0 for k in score_dict} \n",
    "    return {k: (v - min_score) / (max_score - min_score) for k, v in score_dict.items()}\n",
    "\n",
    "def hybrid_search(query, dataset, bert_embeddings, top_n=5, alpha=0.6):\n",
    "    bm25_results = bm25_search(query, dataset, top_n=100)\n",
    "    bert_results = bert_search(query, dataset, bert_embeddings, top_n=100)\n",
    "\n",
    "    bm25_scores = {res['question']: res['score'] for res in bm25_results}\n",
    "    bert_scores = {res['question']: res['score'] for res in bert_results}\n",
    "    bm25_scores_norm = normalize_scores(bm25_scores)\n",
    "    bert_scores_norm = normalize_scores(bert_scores)\n",
    "\n",
    "    all_questions = set(bm25_scores.keys()) | set(bert_scores.keys())\n",
    "\n",
    "    hybrid_results = []\n",
    "    for question in all_questions:\n",
    "        bm_score = bm25_scores_norm.get(question, 0.0)\n",
    "        bert_score = bert_scores_norm.get(question, 0.0)\n",
    "        hybrid_score = round(alpha * bm_score + (1 - alpha) * bert_score, 4)\n",
    "\n",
    "        answer = next(\n",
    "            (res['answer'] for res in bm25_results if res['question'] == question),\n",
    "            next((res['answer'] for res in bert_results if res['question'] == question), \"\")\n",
    "        )\n",
    "\n",
    "        hybrid_results.append({\n",
    "            \"score\": hybrid_score,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "    hybrid_results = sorted(hybrid_results, key=lambda x: -x[\"score\"])[:top_n]\n",
    "    return hybrid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492d9ec8-b284-450e-8ad7-d7b752d90c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top results for query: 'Who was Abraham Lincoln?'\n",
      "[0.8500000238418579] Q: Was Abraham Lincoln the first President of the United States? → A: No\n",
      "[0.8100000023841858] Q: Was Abraham Lincoln the sixteenth President of the United States? → A: yes\n",
      "[0.7900000214576721] Q: Did Abraham Lincoln live in the Frontier? → A: Yes\n",
      "[0.7900000214576721] Q: When did Lincoln first serve as President? → A: March 4, 1861\n",
      "[0.7799999713897705] Q: Who assassinated Lincoln? → A: John Wilkes Booth\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Who was Abraham Lincoln?\"\n",
    "    # results = bm25_search(query, dataset, top_n=5)\n",
    "    results = bert_search(query, dataset, embeddings, top_n=5)\n",
    "    # results = hybrid_search(query, dataset, bert_embeddings=embeddings, top_n=5)\n",
    "\n",
    "    print(f\"\\nTop results for query: '{query}'\")\n",
    "    for res in results:\n",
    "        print(f\"[{res['score']}] Q: {res['question']} → A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e353ff6e-3aa4-4f1b-9c3d-c4e7dfde552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b942fb-2c8e-4d38-a9aa-1cb814a0dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch(batch):\n",
    "    return {\n",
    "        \"instruction\": [\"Answer the question.\"] * len(batch[\"question\"]),\n",
    "        \"input\": batch[\"question\"],\n",
    "        \"output\": batch[\"answer\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(convert_batch, batched=True, batch_size=32)\n",
    "\n",
    "def preprocess(example):\n",
    "    input_texts = [f\"{instr} {inp}\" for instr, inp in zip(example['instruction'], example['input'])]\n",
    "    model_inputs = tokenizer(input_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    labels = tokenizer(example[\"output\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, batch_size=32, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610a066c-f1d0-4c76-b564-3bca50f608f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./flan-t5-lora-rag\",\n",
    "#     learning_rate=5e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_steps=10,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7f494d-25df-441f-a4ea-7bf8946a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"flan-t5-lora-trained\")\n",
    "# tokenizer.save_pretrained(\"flan-t5-lora-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07100265-7771-47e4-8706-e90f01afa87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Scripts\\anaconda_projects\\DocSearch\\venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.01, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name = \"flan-t5-lora-trained\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(model, \"flan-t5-lora-trained\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"flan-t5-lora-trained\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e601b297-5bdf-4fc1-b55b-60cd7778954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(question, retriever=\"hybrid\", top_n=5, dataset=None, embeddings=None):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if retriever == \"bm25\":\n",
    "        results = bm25_search(question, dataset, top_n)\n",
    "    elif retriever == \"bert\":\n",
    "        results = bert_search(question, dataset, embeddings, top_n)\n",
    "    else:\n",
    "        results = hybrid_search(question, dataset, embeddings, top_n=top_n)\n",
    "\n",
    "    context = \"\\n\".join([f\"Q: {r['question']} A: {r['answer']}\" for r in results])\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=100)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    input_text = f\"{prompt} {answer}\"\n",
    "    inputs_ppl = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs_ppl = model(**inputs_ppl, labels=inputs_ppl[\"input_ids\"])\n",
    "    loss = outputs_ppl.loss.item()\n",
    "    perplexity = math.exp(loss)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"time\": round(time.time() - start_time, 2),\n",
    "        \"perplexity\": round(perplexity, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43ff3cbf-1fa4-465c-8ce6-78608f613f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'President of the United States', 'time': 1.48, 'perplexity': 3.07}\n",
      "{'answer': 'President of the United States', 'time': 1.13, 'perplexity': 3.21}\n",
      "{'answer': 'President of the United States', 'time': 1.43, 'perplexity': 3.32}\n"
     ]
    }
   ],
   "source": [
    "question = \"Answer the question: Who was Abraham Lincoln?\"\n",
    "result_bm = ask_llm(question, retriever=\"bm25\", top_n=5, dataset=dataset, embeddings=embeddings)\n",
    "result_bert = ask_llm(question, retriever=\"bert\", top_n=5, dataset=dataset, embeddings=embeddings)\n",
    "result_hybrid = ask_llm(question, retriever=\"hybrid\", top_n=5, dataset=dataset, embeddings=embeddings)\n",
    "print(result_bm)\n",
    "print(result_bert)\n",
    "print(result_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94d0b338-c369-4981-bcc8-f4e75ec95029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'President of the United States', 'time': 1.28, 'perplexity': 7.08}\n",
      "{'answer': 'President of the United States', 'time': 1.12, 'perplexity': 9.13}\n",
      "{'answer': 'President of the United States', 'time': 1.55, 'perplexity': 9.13}\n"
     ]
    }
   ],
   "source": [
    "result_bm = ask_llm(question, retriever=\"bm25\", top_n=1, dataset=dataset, embeddings=embeddings)\n",
    "result_bert = ask_llm(question, retriever=\"bert\", top_n=1, dataset=dataset, embeddings=embeddings)\n",
    "result_hybrid = ask_llm(question, retriever=\"hybrid\", top_n=1, dataset=dataset, embeddings=embeddings)\n",
    "print(result_bm)\n",
    "print(result_bert)\n",
    "print(result_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b349efc-6d2d-4cb7-a63f-2bae1c48451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'President of the United States', 'time': 2.07, 'perplexity': 2.51}\n",
      "{'answer': 'President of the United States', 'time': 2.0, 'perplexity': 2.57}\n",
      "{'answer': 'President of the United States', 'time': 2.23, 'perplexity': 2.47}\n"
     ]
    }
   ],
   "source": [
    "result_bm = ask_llm(question, retriever=\"bm25\", top_n=10, dataset=dataset, embeddings=embeddings)\n",
    "result_bert = ask_llm(question, retriever=\"bert\", top_n=10, dataset=dataset, embeddings=embeddings)\n",
    "result_hybrid = ask_llm(question, retriever=\"hybrid\", top_n=10, dataset=dataset, embeddings=embeddings)\n",
    "print(result_bm)\n",
    "print(result_bert)\n",
    "print(result_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e98875c4-5a9e-425b-b696-807fdc2b7037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'A: no', 'time': 3.2, 'perplexity': 1.97}\n",
      "{'answer': 'A: Millard Fillmore', 'time': 3.66, 'perplexity': 2.05}\n",
      "{'answer': 'A: George Washington', 'time': 3.78, 'perplexity': 2.03}\n"
     ]
    }
   ],
   "source": [
    "result_bm = ask_llm(question, retriever=\"bm25\", top_n=100, dataset=dataset, embeddings=embeddings)\n",
    "result_bert = ask_llm(question, retriever=\"bert\", top_n=100, dataset=dataset, embeddings=embeddings)\n",
    "result_hybrid = ask_llm(question, retriever=\"hybrid\", top_n=100, dataset=dataset, embeddings=embeddings)\n",
    "print(result_bm)\n",
    "print(result_bert)\n",
    "print(result_hybrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DocSearch)",
   "language": "python",
   "name": "docsearch-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
